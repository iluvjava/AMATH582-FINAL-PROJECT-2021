\documentclass{article}

% If you're new to LaTeX, here's some short tutorials:
% https://www.overleaf.com/learn/latex/Learn_LaTeX_in_30_minutes
% https://en.wikibooks.org/wiki/LaTeX/Basics

% Formatting
\usepackage[utf8]{inputenc}
\usepackage[margin=1in]{geometry}
\usepackage[titletoc,title]{appendix}

% Math
% https://www.overleaf.com/learn/latex/Mathematical_expressions
% https://en.wikibooks.org/wiki/LaTeX/Mathematics
\usepackage{amsmath,amsfonts,amssymb,mathtools}

% Images
% https://www.overleaf.com/learn/latex/Inserting_Images
% https://en.wikibooks.org/wiki/LaTeX/Floats,_Figures_and_Captions
\usepackage{graphicx,float}

% Tables
% https://www.overleaf.com/learn/latex/Tables
% https://en.wikibooks.org/wiki/LaTeX/Tables

% Algorithms
% https://www.overleaf.com/learn/latex/algorithms
% https://en.wikibooks.org/wiki/LaTeX/Algorithms
\usepackage[ruled,vlined]{algorithm2e}
\usepackage{algorithmic}

% Code syntax highlighting
% https://www.overleaf.com/learn/latex/Code_Highlighting_with_minted
% \usepackage{minted}
%\usemintedstyle{borland}

% References
% https://www.overleaf.com/learn/latex/Bibliography_management_in_LaTeX
% https://en.wikibooks.org/wiki/LaTeX/Bibliography_Management
\usepackage{biblatex}
\addbibresource{references.bib}

\usepackage{hyperref}
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,      
    urlcolor=cyan,
}

% Title content
\title{TITLE}
\author{Hongda Li}
\date{\today}

\begin{document}

\maketitle

% Abstract
\begin{abstract}
   ...
\end{abstract}


\section{Introduction and Overview}


\section{Theoretical Background}
    \hspace{1.1em}
    In this section we will go through several of the popular machine learning model and discuss the mathematics behind these models. The presentation is made hope to be informative about the mathematical background of the models, and how the mathematical understanding of them lead to a better understanding of the structure and topology of the data.Our hypothesis is that, the performance Neuro Net works, or Convolutional Neuro Net work is as good as classical models if not better.  
    \subsection{Data Standardization}
        \hspace{1.1em}
        Data standardization is used for classical training method. This is necessary because models such as the SVM, LDA, PCA, and Decision Tree has biases depending on the size of the samples for each of the classes. Hence, all the data is Standardized by flattening the image of the data, and for all features of each sample is set to zero mean and unit variance. In addition, for training SVM and Decision tree, there are exact same number of samples presented for each of the labels. 
    \subsection{Dimensionality Reduction Using Fisher's LDA}
        \hspace{1.1em}
        Principal Component Analysis and Linear Discriminant Analysis are both used together to create low dimensional embeddings of the feature space. The original feature space for each of the images lie in a 784 dimension because each pixel of the image is considered as a feature and the image has size $28 \times 28$. 
        \par
        LDA is both a classifier and a dimensionality reduction technique, here we introduce Fisher's LDA, the idea is to look for a subspace (Not necessary orthogonal) such that, the embeddings of all the samples in the training set has maximal spread and minimal deviance among all the classes labels. One a high level, the problem is phrased as: 
        \begin{equation*}\tag{1}\label{eqn:1}
            \max_{\Vert x\Vert = 1} \left\lbrace
                \frac{v^TS_{\text{Between}}v}{v^TS_{\text{Within}}v}
            \right\rbrace
        \end{equation*}
        Where 2 of the matrices are defined as: 
        \begin{align*}\tag{2}\label{eqn:2}
            S_{\text{between}} = \sum_{i = 1}^{k}\left(
                (u_i - \mu)(u_i - \mu)^T
            \right) \quad 
            \\
            S_{\text{within}} =
            \sum_{i=1}^{k}\left(
                \sum_{x_i\in C_i}^{}
                    (x_i - m_i)(x_i - m_i)^T
            \right)
        \end{align*}
        Where quantity $\mu_i$ denotes the average of the class $C_i$ projected onto the chosen subspace $v$, and the quantity $m_i$ is the centroid of all the samples for the class $C_i$: 
        \begin{equation*}\tag{3}\label{eqn:3}
            \mu_i = \frac{1}{|C_i|}\sum_{x\in C_i}v^Tx_i
            \quad\quad
            \mu = \frac{1}{k}\sum_{i =1}^{k}\mu_i
        \end{equation*}
        \begin{equation*}\tag{4}\label{eqn:4}
            m_i = \sum_{x_i\in C_i}^{}
                x_i
        \end{equation*}
        Notice that equation \hyperref{eqn:1}{Expression 1} is the key here, the solution to the optimization problem is Eigenvector of the maximal Eigenvalue for the matrix $S_{\text{within}}^{\dagger}S_{\text{between}}$, however, the solution is not unique, all Eigenvector can be projected onto, and this is the subspace of embeddings for the LDA. 
        \par
        However, do notice that taking the inverse of the matrix to be full rank, and LDA can only project the feature space down to $\min(k, n) - 1$ where $k$ is the number of classes and $n$ is the total number of the samples. 
        \par
        However, the drawback for includes the fact that it requires the classes of the labels to have Gaussian Distributions of equal variances across all classes, in addition, it cannot separate non-linear distributions. % CITATION FOR THE LDA?????? 
    \subsection*{PCA Dimensionality Reduction}
        \hspace{1.1em}
        PCA: Principal Component Analysis is an unsupervised learning technique and it's usually used as a dimensionality reduction technique. The Singular Value decomposition decompose any matrix $A$ into the product of 3 matrices: $U\Sigma V^T$, where $U, V^T$ are orthogonal matrices and $\Sigma$ is a diagonal matrix with Singular Values on the diagonal. 
        \par
        In practice, matrix $A$ is the data matrix and the lower dimensional embedding are the $\Sigma V^T$ is the $A$ matrix is the data matrix and $U\Sigma$ if the $A$ matrix is a row matrix. 
        \par
        PCA is very different for one reason, it supports non-linearity and it's based on the idea of explaining variance in the data set. And nothing can demonstrate this idea better than the Low-rank Approximation theorem: 
        \begin{equation*}\tag{5}\label{eqn:5}
            U_k\Sigma_k V^T_k = \underset{\text{rank}(X)=k}{\text{argmin}}  \left\lbrace
                \Vert X - A\Vert_F^2
            \right\rbrace
        \end{equation*}
        \par
        Where the, the $k$ truncated singular value decomposition of matrix $A$ is the solution to the $k$ rank approximation problem under the Frobenius Norm (And it measures the variance across differences between these 2 matrices). 
        \par
        This is incredibly useful in a sense that, if the variance of the data set can explained by some kind of low rank structure, then those structure will be review in the Principal Components, the $U$, or the $V$ matrix. 
        \par
        Principal Components is very useful for describing the data in an orthogonal subspace that is much lower than the original features space where the data is located int. The idea is so powerful that, Randomized SVD and Sparse SVD algorithm is implemented in the Sklearn Library. % CITATION 
        \par
        However, compare to the Fisher's LDA embeddings, PCA is lossy once the truncated SVD is used. In addition, SVD is sensitive to noise and non-linear transformation of the original data, stretched and rotated images will introduce different principal components into the decomposition.
    \subsection*{LDA + PCA for Dimensionality Reduction and Data Mining}
        \hspace{1.1em}
        One of the major idea for training classical models and visualizing the data before training is the usage of combining both the PCA and LDA as a dimensionality reduction method to embed samples in a feature space that has a much lower dimension, speeding up classical algorithm such as the Support Vector Machine, Decision tree or Random Forest. 
        \par
        To get the best out of both algorithm PCA is performed on images partition by labels into equally sized classes and then LDA is performed with the PCA embeddings of the data, this is done using the training set. 
        \par
        When making prediction with a model, test set is projected onto the PCA components and then transformed into the LDA sub-space before feeding into the model for prediction.

    \subsection*{SVM}
        \hspace{1.1em}
        The idea behind SVM: Support Vector Machine is simple and powerful. The problem is formulated as a Quadratic Optimization problem, summarized as the following: 
        \begin{equation*}\tag{6}\label{eqn:6}
            \min_{\beta, \beta_0} 
            \left\lbrace
            \frac{1}{2}
                \left\Vert
                    \beta
                \right\Vert^2
                + C \sum_{i = 1}^{N}\xi_i
            \right\rbrace
            \text{ s.t: } 
            \begin{cases}
                y_i(x_i^T\beta + \beta_0) \ge 1 - \xi_i \quad \forall i
                \\
                \xi_i \ge 0, \sum_{i}^{}\left(\xi_i\right) \le 1 - \xi_i \quad \forall i
            \end{cases}
        \end{equation*}
        The quantity $\beta, \beta_0$ are parameters that define the separating hyperplane, and the slack variables $\xi_i$ are used as a penalty for placing the samples on the wrong side of the hyperplane. In practice, the problem is put into a dual primal format and then optimization with the KTT (Karush-Kuhn-Tucker conditions) are used. 
        \par
        And, buy consider the Fenchel Transform on the primal objective of the function (Which is itself), we obtain the Lagrangian of the system in the following form:
        \begin{equation*}\tag{7}\label{eqn:7}
            \mathcal{L}(\beta, \beta_0, \xi, \alpha) = \frac{1}{2}\Vert \beta\Vert^2
            + 
            C \sum_{i = 1}^{N}\xi_i - 
            \sum_{i = 1}^{N}
                \alpha_i [y_i(x_i^\beta + \beta_0) - (1 - \xi_i)]
            - 
            \sum_{i = 1}^{N}\mu_i \xi_i
        \end{equation*}
        And the constraints of the primal problem is relaxed using positivity assumptions on the variables: $\alpha_i, \mu_i, \xi_i \ge 0 \;\forall i$. 
        \par
        Much more theoretical Perspective of this part is highlighted in the book \textit{Elements of Statistical Learning}, where taking the derivative on the Lagrangian wrt the primal variable ($\beta_i, \xi_i$) will yield the results that: 
        \begin{align*}\tag{8}\label{eqn:8}
            \beta &= \sum_{i = 1}^{N}\alpha_i y_i x_i 
            \\
            0 &= \sum_{i = 1}^{N}\alpha_i y_i
            \\
            \alpha_i &= C - \mu_i \quad \forall i
        \end{align*}
        Often in practice, much noble algorithms are involved in solving the SVM optimal solutions and most of them are put into the dual form because it has a connection to the Kernel Function and inner products o all the features, and it give rise to the use of the Kernel function. 
        \par
        In practice, SVM is very effective under the task of binary classification under small sample but much worse for when then features space is in a very high dimension, and for multi-class classification where many labels are involved. This is the case because of the complexity of the algorithm both dimension and the number of samples used for training are very large, and the complexity is given by $\mathcal{O}(mn^2)$ where $n$ denotes the number of samples and $m$ denotes the number of features. 
        \par
        To overcome, we use the dimensionality technique introduced previous to improve the speed of the SVM. In addition, SVM serves as a great tools for looking into the structure of the classification and highlighting potential difficulties for other machine learning model, it serves as a ``Lower Bound'' for how well other machine learning model can perform. Finally, via experiments, it's a model that hardly over-fit and under-fit is indicative of potential irreducible errors on the data.

    \subsection*{Decision Tree and Random Forest}
    % This part cite the medium article about the information gain for the decision tree and decision tree training. 
        \hspace{1.1em}
        The idea behind decision tree is yet, another simple and powerful idea. There are 2 type of decision tree, regression tree where it seeks to approximate a function and the classification tree for classifying the samples with labels. 
        \par
        Under the most basic premise, each node of the of the tree, it split the samples into 2 sets trying to maximizes the information gain when splitting. The entropy of the tree can be measured by the expression: 
        \begin{equation*}\tag{9}\label{eqn:9}
            E(p) = \sum_{i = 1}^{k}-p_i\log(p_i)
        \end{equation*}
        Where the quantity $p_i$ denotes the portion of samples with label $i$ and $k$ denotes the total number of samples in the current node of the tree. Observe that the function reaches maximum when one of the labels saturated the node of the tree. 
        \par
        To make a branch on the tree, a separation criterion is made on one of the feature, and it seeks to maximizes the information gain, which can be summarized as one expressions: 
        \begin{align*}\tag{10}\label{eqn:10}
            E(\text{Parent}) -
            \frac{N_{<k}}{N}E(F_1 < k) - \frac{N_{\ge k}}{N}E(F_1\ge k)
        \end{align*}
        Where, the number $N_{<k}$ is the total number of samples in the left node of the parent node and $N_{\ge k}$ is the total number of samples in the right node. And the expression $E(F_1 < k)$ denotes the condition energy of the left node with all the sample where their feature $F_1$ satisfies the criterion: $<k$, it's the same story on the right side of the node. 
        \par
        The simplicity of the decision tree gives it very large variance on the model, which means that, depending on which features are chosen to split and the parameters, many different types of tree can be configured. In general, the construction of the optimal tree is an NP-Hard problem, hence usually heuristics are used for constructing the tree given a training sample. % CITATION NEEDED. 
        \par
        However, the power of tree is manifested by a forest. The idea is to use bootstrap to train multiple tree and use the idea of bagging, where we either takes the average of the prediction from the group of trees trained from the data (Committee Method), or we weight their votes using their by penalizing trees that are more complex (Stacked Generalization). When the tree is a classifier, we simply take the votes to produce the predicted label. 
        % Statistical Learning 288
        \par
        Nonetheless, one of the major practical advantage of the Random Forest Classifier over the SVM is the speed of training. The algorithm allows the tree to be train in concurrently, reducing the amount of time needed to train the model. 
    \subsection*{Logistic Regression, Softmax Regression}
        \hspace{1.1em}
        The logistic regression is a generalized linear model. It's derived from using the idea of a Maximal Likelihood Estimator and Linear Regression. However, under its most general form, it's presented as a softmax regression for multi-class classification tasks. 
        \par
        Under the multi-class classification, we consider the generalization problem with $\{x_i, y_i\}$ where $x_i\in \mathbb{R}^n$ and $y_i \in \{1,2, \cdots , K\}$. 
        \par
        And the hypothesis of given an observation and a lits of parameters will be given in the form: 
        \begin{equation*}\tag{11}\label{eqn:11}
            h_\Theta(x) = 
            \frac{1}{\sum_{j = 1}^{K}\exp(\theta_j^Tx)}
            \begin{bmatrix}
                \exp(\theta_1^Tx) \\ 
                \exp(\theta_2^Tx) \\
                \vdots
                \\
                \exp(\theta_K^Tx)
            \end{bmatrix}
        \end{equation*}
        And the parameters are in the form of a matrix $\Theta$ where $\theta_i$ are the columns of the matrix and the matrix will be $x\times K$. 
        \par
        In practice, we take the logarithm and linearize the MLE, and gradient descend are usually used to compute the optimal parameters. Due to the fact that the model itself is a generalize linear model, it inherits regularization options from linear regression, such as L1, L2 or elastic nets. 
        
    \subsection*{Neuro Network}

    \subsection*{Performance Metric}
        To measure the performance of the models, confusion matrix is used. The confusion matrices 


\section{Algorithm Implementation and Development}


\section{Computational Results}
    

\section{Summary and Conclusions}
   
% References
\printbibliography

% Appendices
\begin{appendices}

\section{MATLAB Functions}

    
\end{appendices}

\end{document}
